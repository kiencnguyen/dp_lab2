{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "from qdrant_client.http import models\n",
    "from qdrant_client.http.models import PointStruct\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_FILE = \"\"\n",
    "QDRANT_COLLECTION = \"\"\n",
    "OPENAI_EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "OPENAI_GPT_MODEL = \"gpt-3.5-turbo\"\n",
    "\n",
    "# Maximum number of results for Qdrant vector database\n",
    "MAX_NO_SEARCH_RESULTS_QDRANT = 5\n",
    "# Used to split the pdf file\n",
    "CHUNK_SIZE = 500\n",
    "SECRETS_DIRECTORY = \"../secrets\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting URLs and API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI API key file\n",
    "openai_api_key_file = os.path.join(SECRETS_DIRECTORY, 'openai_api_key.secret')\n",
    "\n",
    "# Qdrant URL file\n",
    "qdrant_url_file = os.path.join(SECRETS_DIRECTORY, 'qdrant_url.secret')\n",
    "\n",
    "# Qdrant API key file \n",
    "qdrant_api_key_file = os.path.join(SECRETS_DIRECTORY, 'qdrant_api_key.secret') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_key(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            key = file.read().strip()\n",
    "        return key\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{file_path}' was not found.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the file '{file_path}': {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# OpenAI API key\n",
    "OPENAI_API_KEY = read_key(openai_api_key_file)\n",
    "\n",
    "# Qdrant URL\n",
    "QDRANT_URL = read_key(qdrant_url_file)\n",
    "\n",
    "# Read the API key from the file\n",
    "QDRANT_API_KEY = read_key(qdrant_api_key_file)\n",
    "\n",
    "print(QDRANT_URL, \"\\n\", QDRANT_API_KEY, \"\\n\", OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Showing Qdrant collection information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_client = QdrantClient(\n",
    "    host=QDRANT_URL,\n",
    "    api_key=QDRANT_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qdrant_client.recreate_collection(\n",
    "#     collection_name=QDRANT_COLLECTION,\n",
    "#     vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE),\n",
    "# )\n",
    "# print(\"Create collection reponse:\", qdrant_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_info = qdrant_client.get_collection(collection_name=QDRANT_COLLECTION)\n",
    "print(\"Collection info:\", collection_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read PDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = \"\"\n",
    "with pdfplumber.open(PDF_FILE) as pdf:\n",
    "    # Going through all pages\n",
    "    for page in pdf.pages:\n",
    "        full_text += page.extract_text()\n",
    "\n",
    "print(full_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunk PDF text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = full_text\n",
    "\n",
    "chunks = []\n",
    "# The loop continues as long as the length of text is greater than 500 characters\n",
    "while len(text) > CHUNK_SIZE:\n",
    "    # Find the last period within the first 500 characters\n",
    "    last_period_index = text[:CHUNK_SIZE].rfind('.')\n",
    "    if last_period_index == -1: \n",
    "        # If there's no period, then we get the whole CHUNK_SIZE\n",
    "        last_period_index = CHUNK_SIZE\n",
    "    chunks.append(text[:last_period_index])\n",
    "    # Moving the the next chunk\n",
    "    text = text[last_period_index+1:]\n",
    "chunks.append(text)\n",
    "\n",
    "# for chunk in chunks:\n",
    "#     print(chunk)\n",
    "#     print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create embeddings and index with qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = []\n",
    "i = 1\n",
    "for chunk in chunks: \n",
    "    i += 1\n",
    "\n",
    "    print(\"Embeddings chunk: \\n\", chunk)\n",
    "    openai_client = openai.OpenAI(api_key=OPENAI_API_KEY)\n",
    "    embedding_response = openai_client.embeddings.create(\n",
    "        input=chunk,\n",
    "        model=OPENAI_EMBEDDING_MODEL\n",
    "    )\n",
    "\n",
    "    print(embedding_response)\n",
    "\n",
    "    embedding = embedding_response.data[0].embedding   \n",
    "    points.append(PointStruct(id=i, vector=embedding, payload={\"text\": chunk}))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# class PromptServices (adapted from class PromptServices in FastAPI code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making prompt_services global so we don't need to construct this object for every query\n",
    "prompt_services = PromptServices(\n",
    "    OPENAI_API_KEY,\n",
    "    QDRANT_URL,\n",
    "    QDRANT_API_KEY,\n",
    "    OPENAI_EMBEDDING_MODEL,\n",
    "    OPENAI_GPT_MODEL\n",
    ")\n",
    "\n",
    "class PromptServices:\n",
    "    \"\"\"\n",
    "    Class handling various prompt services related operations.\n",
    "    \"\"\"\n",
    "\n",
    "    # Constructor\n",
    "    def __init__(self,\n",
    "                 openai_api_key,\n",
    "                 qdrant_url,\n",
    "                 qdrant_api_key,\n",
    "                 openai_embedding_model,\n",
    "                 openai_gpt_model):\n",
    "\n",
    "        self._open_api_key = openai_api_key\n",
    "        self._qdrant_url = qdrant_url\n",
    "        self._qdrant_api_key = qdrant_api_key\n",
    "        self._openai_embedding_model = openai_embedding_model\n",
    "        self._openai_gpt_model = openai_gpt_model\n",
    "\n",
    "        self._openai_client = openai.OpenAI(api_key=self._open_api_key)\n",
    "        self._qdrant_client = QdrantClient(host=self._qdrant_url,\n",
    "                                           api_key=self._qdrant_api_key)\n",
    "        self._system_prompt = (\"You are a knowledgeable assistant. \"\n",
    "                               \"Please use the provided context to answer the question. \"\n",
    "                               \"Please be as helpful and relevant as possible. \"\n",
    "                               \"If you do not have the information, \"\n",
    "                               \"please do not make up the answer.\")\n",
    "\n",
    "    #############################################\n",
    "    # Get the embedding of the query using the provided embedding model\n",
    "    def get_embedding(self, query):\n",
    "\n",
    "        # Get the embedding of the query\n",
    "        try:\n",
    "            embedding_response = self._openai_client.embeddings.create(\n",
    "                input=query,\n",
    "                model=self._openai_embedding_model\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(\"Exception :\", str(e))\n",
    "\n",
    "        return embedding_response.data[0].embedding\n",
    "\n",
    "    #############################################\n",
    "    # Search for closest texts in Qdrant vector database\n",
    "    def get_context(self, embedding):\n",
    "\n",
    "        try:\n",
    "            search_results = self._qdrant_client.search(\n",
    "                collection_name=QDRANT_COLLECTION,\n",
    "                query_vector=embedding,\n",
    "                limit=MAX_NO_SEARCH_RESULTS_QDRANT\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(\"Exception :\", str(e))\n",
    "\n",
    "        return search_results\n",
    "\n",
    "    #############################################\n",
    "    # Search for closest texts in Qdrant vector database\n",
    "    def get_context_2(self, embedding):\n",
    "\n",
    "        try:\n",
    "            search_results = self._qdrant_client.search(\n",
    "                collection_name=QDRANT_COLLECTION,\n",
    "                query_vector=embedding,\n",
    "                limit=1\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(\"Exception :\", str(e))\n",
    "\n",
    "        return search_results   \n",
    "    #############################################\n",
    "    def get_response(self, query, search_results):\n",
    "\n",
    "        context = \"\"\n",
    "        for result in search_results:\n",
    "            context += result.payload['text'] + \"\\n\"\n",
    "\n",
    "        try:\n",
    "            chat_response = self._openai_client.chat.completions.create(\n",
    "                model=self._openai_gpt_model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": self._system_prompt},\n",
    "                    {\"role\": \"assistant\", \"content\": context},\n",
    "                    {\"role\": \"user\", \"content\": query}\n",
    "                ]\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(\"Exception :\", str(e))\n",
    "\n",
    "        return chat_response.choices[0].message.content.strip()\n",
    "\n",
    "\n",
    "    #############################################\n",
    "    def get_response_2(self, query, search_results):\n",
    "\n",
    "        context = \"\"\n",
    "        qdrant_id = search_results[0].id\n",
    "        print(qdrant_id)\n",
    "\n",
    "\n",
    "        search_results = qdrant_client.retrieve(\n",
    "            collection_name=QDRANT_COLLECTION,\n",
    "            ids=list(range(max(2, qdrant_id-2), qdrant_id+3))\n",
    "        )\n",
    "\n",
    "        print(list(range(max(2, qdrant_id-2), qdrant_id+3)))\n",
    "        \n",
    "        for result in search_results:\n",
    "            context += result.payload['text'] + \"\\n\"\n",
    "\n",
    "        try:\n",
    "            chat_response = self._openai_client.chat.completions.create(\n",
    "                model=self._openai_gpt_model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": self._system_prompt},\n",
    "                    {\"role\": \"assistant\", \"content\": context},\n",
    "                    {\"role\": \"user\", \"content\": query}\n",
    "                ]\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(\"Exception :\", str(e))\n",
    "\n",
    "        return chat_response.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get_answer_from_llm() - From FastAPI code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_from_llm(query):\n",
    "    try:\n",
    "        embedding = prompt_services.get_embedding(query)\n",
    "        search_results = prompt_services.get_context(embedding)\n",
    "        response = prompt_services.get_response(query, search_results)\n",
    "    except Exception as e:\n",
    "        print(\"Exception :\", str(e))\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_from_llm_2(query):\n",
    "    try:\n",
    "        embedding = prompt_services.get_embedding(query)\n",
    "        search_results = prompt_services.get_context_2(embedding)\n",
    "        response = prompt_services.get_response_2(query, search_results)\n",
    "    except Exception as e:\n",
    "        print(\"Exception :\", str(e))\n",
    "\n",
    "    return response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
